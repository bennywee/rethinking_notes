---
title: "Textbook Code"
author: "Benjamin Wee"
output: pdf_document
---

# Textbook Code

## Chapter 2

```{r}
# Page 40
# Define Grid
p_grid <- seq(from = 0, to =1, length.out = 20)

# Define prior
#prior <- rep(1, 20)
prior <- ifelse(p_grid<0.5, 0, 1)
#prior <- exp(-5*abs(p_grid - 0.5))

# Compute likelihood at each value in grid
likelihood <- dbinom(6, size = 9, prob = p_grid)

# Compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# Standardise the posterior so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# Visualise posterior
plot( p_grid , posterior , type="b" ,
xlab="probability of water" , ylab="posterior probability" )
mtext( "20 points" )

```

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# Page 42
library(rethinking)
globe.qa <- map(
  alist(
    w ~ dbinom(9, p), # Binomial Likelihood
    p ~ dunif(0,1)    # Uniform Prior
  ),
  data = list(w = 6)
)

# Display summary of quadratic approximation
precis(globe.qa)
```
```{r}
# Page 43
# Analytical Calculation
w <- 6
n <- 9
curve(dbeta(x, w+1, n-w+1), from = 0, to = 1)

# Quadratic approximation
curve(dnorm(x, 0.67, 0.16), lty = 2, add = TRUE)
```

## Chapter 3 Sampling the imaginary

Bayesian inference is distinguised by a broad view of probability. Not by use of Bayes' therorem.

### 3.1 Sampling from a grid-approximate posterior
```{r}
# Page 49
# Conditional Probability Calculation

PrPV <- 0.95
PrPM <- 0.01
PrV <- 0.001
PrP <- PrPV*PrV + PrPM*(1-PrV)
( PrVP <- PrPV*PrV / PrP )
```

```{r}
# Page 52
# Sampling from a grid approximate posterior
p_grid <- seq( from=0 , to=1 , length.out=1000 )
prior <- rep( 1 , 1000 )
likelihood <- dbinom( 6 , size=9 , prob=p_grid )
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)

samples <- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE)
plot(samples)

# Density approximiation of samples
library(rethinking)
dens(samples)
```

### 3.2 Sampling to summarise

#### 3.2.1. Intervals of defined boundries
```{r}
# add up posterior probability where p < 0.5
sum(posterior[ p_grid < 0.5 ]) # So about 17% of posterior probability is under 0.5
```

```{r}
# Performing same calculation from samples of the posterior
sum(samples<0.5) / 1e4 # About the same
```

```{r}
# See how much probability is between 0.5 and 0.75
sum( samples > 0.5 & samples < 0.75 ) / 1e4
```

#### 3.2.2. Intervals of defined mass

It's easy to keep track of what's being summarised as long as you pay attention to how the model is defined. We can use sampling to get the boundries of the *lower* 80% posterior probability. 

```{r}
# R code 3.9
quantile(samples, 0.8)
```

Similarly the middle 80% interval lies between the 10th and 90th percentile. Using sampling:

```{r}
# R code 3.10
quantile(samples, c(0.1, 0.9))
```

These are known as **percentile intervals (PI)** -- good for summarising symmertical distributions. They are limited for assymetrical distributions and are _not_ perfect for for supporting inferences about which parameters are consistent with the data.

The posterior distribution is consistent with observing 3 waters in 3 tosses with a uniform prior. This is highly skewed with a maximum boundry balue of $p = 1$. Computing this with grid approximation: 

```{r}
# R code 3.11 - Grid approximation of posterior + sampling
p_grid <- seq(from = 0, to = 1, length.out = 1000)
prior <- rep(1, 1000)
likelihood <- dbinom(3, size = 3, prob = p_grid)
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
samples <- sample(p_grid, size = 1e4, replace = TRUE, prob = posterior)
```

Computing the 50% percentile intervals:

```{r}
# R code 3.12: The interval assigns 25% probability mass above and below the interval.
PI(samples, prob = 0.5)
```
But this example excludes the most probably parameter value, $p = 1$. Therefore, the PI is misleading in describing the shape of this posterior. 

The Highest Posterior Density Interval (HPDI) -- narrowest interval containing the specified probability mass. 

```{r}
# R code 3.13 - HPDI - smallest interval with 50% probability mass
HPDI(samples, prob = 0.5)
```

HPDI has some advantages over PI, but in many cases they are similar. In this case, the intervals are different due to a *highly skewed distribution*. Using different probability masses:

```{r}
HPDI(samples, prob = 0.8)
HPDI(samples, prob = 0.95)
```

Disadvantages of HPDI -- More computationally intensive than PI and greater simulation variance (sensitive to how many samples are drawn from the posterior).

Overall, if the choice of interval type makes a big difference, then we *shouldn't* be using intervals to summarise the posterior. Better of plotting the estimate of the posterior distribution.

#### 3.2.3. Point Estimates

*Maximum a posteriori* (MAP) estimate -- parameter with the highest posterior probability.

```{r}
# R code 3.14 -- MAP
p_grid[which.max(posterior)]
```

You can approximate the point if you have samples from the posterior:

```{r}
# R code 3.15
chainmode(samples, adj = 0.01)
```

Could also report posterior mean and median:

```{r}
# R code 3.16
mean(samples)
median(samples)
```

But how do we choose between MAP, median and mean?

We could use a **loss function**. This is a rule which gives the cost associated with using any point estimate. *Different loss functions imply different point estimates*. 
Calculating the loss for any point estimate means using the posterior to average over our uncertainty in the true value. If we arbitrarily choose 0.5 as our point estimate, the *expected loss* will be:

```{r}
# R code 3.17
# Sum of posterior probability multiplied by deviation of point estimate from all other possible parameter values of p. This is *averaging* over the uncertainty of the posterior distribution. The posterior in this case gives some probabilistic weight to each parameter value in p_grid, some more or less than others. 

# Goal is to *minimise* this value (minimising the weighted average loss).
sum(posterior * abs(0.5 - p_grid)) 

# sum(weight_i * abs(point_estimate - parametervalues_i))
```

We can repeat this calculation for each decision and choose the smallest value. 

```{r}
# R code 3.18.
loss <- sapply(p_grid, function(d) sum(posterior*abs(d-p_grid)))
```

```{r}
# R code 3.19 - Find parameter value which minimises the loss
p_grid[which.min(loss)] # same as posterior median in this case. 
```

In principle, the details of the applied context may demand unique loss functions. Some loss functions may be highly assymetric. Rather, point estimates should help *describe* the shape of the posterior distribution. 

### 3.3 Sampling to simulate predction

 1) *Model checking* -- Simulating implied observations to check if the model fit is correct and to investigate model behaviour. 
 
 2) *Software validation* -- Simulate observations under a known model and then attempt to recovr the values of the parameters the data were simulated under. 
 
 3) *Research design* -- If you can simulate observations from your hypothesis, then you can evaluate whether the research design can b effective. In a narrow sense, this means doing *power analysis*, but the possibilties are much broader. 
 
 4) *Forecasting* -- Estimates can be used to simulate new predictions, for new cases and future observations. These forecasts can be useful as applied prediction, but also for model criticism and revision.

#### 3.3.1. Dummy Data

A fixed 'true' proportion of water p exists, and that is the target of our inference. 
So far, our assumptions have allowed us to infer the plausilibilty of each possible value of *p* after observations. Using the same assumptions, we can simulate the observations that the model implies, since the likelihood works in both directions. 

Given a realised observation, the likelihood function says how plausible an observation is. 

Given only the parameters, the likelihood defines a distribution of possible observations that we can sample from -- to simulate observations. 

Therefore, bayesian models are *always* **generative** -- capable of simulating predictions. 

Using a binomial likelihood:

$$
Pr(w|n,p) = \frac{n!}{w!(n-w)!}p^w(1-p)^{n-w} \\
n = 2, p = 0.7
$$

Computing the likelihood:

```{r}
# R code 3.20
dbinom(0:2, size = 2, prob = 0.7)
```

9% chance of observing $w = 0$, 42% chance of observing $w = 1$, 42% chance of $w = 2$. 

Simulating observations from the likelihood:

```{r}
#R code 3.21
rbinom(1, size = 2, prob = 0.7)

#R code 3.22
rbinom(10, size = 2, prob = 0.7)
```

```{r}
# R code 3.23
dummy_w <- rbinom(1e5, size = 2, prob = 0.7)
table(dummy_w)/1e5
```

```{r}
# R code 3.24 -- Extending this to 9 tosses
dummy_w <- rbinom(1e5, size = 9, prob = 0.7)
simplehist(dummy_w, xlab = "dummy water count")
```

#### 3.3.2. Model Checking

Model checking means:

  1) Ensuring model fitting worked correctly
  2) Evaluating the adequacy of a model for some purpose
  
Bayesian models are *generative*: able to simulate observaions and estimate parameters from observations. Once the model is conditioned on data, you can simulate to examine the model's *empirical expectations*.


  3.3.2.1. *Did the software work?* -- Check implied predictions and the data used to fit the model (retrodictions). How well does the model reproduce the data used to educate it. There should be *some* but not *exact* correspondence. Otherwise this may suggest something wrong with the software. This is a simple check to catch silly mistakes since there's no sure way to check that the software is working correctly. 
  
  3.3.2.2. *Is the model adequate?* -- Looking for aspects of the data that are not described well by the model's expectations. The goal is *not* to test whether the model's assumptions are "true", because all models are false. Rather, the goal is to assess exactly how the model fails to describe the data, as a path towards model comprehension, revision and improvement. 
  All models will fail in some aspect -- use own judgement and others -- to decide whether particular failure is important or not. 
  Sample from entire posterior distribution, not just a point estimate. Posterior contains lots of infromation about uncertainty. Stripping this away leads to overconfidence. 

Need to be aware of uncertainty from the implied predictions of the model.

  1) Observation uncertainty -- For each unique parameter value p, there is a unique implied pattern of observations that the model expects. There is uncertainty in the predicted observations, since even if you know p with certainty, you won't know the next globe toss with certainty. 
  
  2) Uncertainty about the parameter p. The posterior over p embodies this uncertainty. Since there is uncertainty about p, there is uncertainty about everything that depends upon p.  The uncertainty in p will interact with the sampling variation, when we try to assess what the model tells us about outcomes. 
  
We want to *propagate* parameter uncertainty -- carriy it forward -- as we evaluate the implied predictions. For each value of parameter *p*, there is an implied distributions of outcomes. We can average all these predictive distributions together using the posterior probabilities of each value of *p* to get a **posterior predictive distribution**. 

*Posterior predictive distribution* - Compute the sampling distribution of each parameter value p and average over all these predictive distributions together using the posterior probabilities. 

The resulting distributions is for predictions, but it incorperates all of the uncertainty embodied in the posterior distribution for the parameter *p*. As a result, it is honest. 

If you use only a single parameter value to compute implied predictions, say the most probable value at the peak of the distribution, you'd produce an *overconfident distribution of predictions*. This will lead you to believe the model is more consistent with the data than it really is -- predictions will cluster around the obesrvations more tightly. This illusion arises from tossing away uncertainty about the parameters.

To see this:

```{r}
# R code 3.25
# Generates 10,000 simulated predictions of 9 globe tosses assuming p = 0.6
w <- rbinom(1e4, size = 9, prob = 0.6)
simplehist(w)
```

```{r}
# R code 3.26
# Sampled values appear in proportion to their posterior probabilities -- the resulting simulated observations are averaged over the posterior. 
w <- rbinom(1e4, size = 9, prob = samples)
simplehist(w)
```
Can also consider distributions of longest run of waters or number of switches within 9 globe tosses.

Models are always wrong, in some sense *mis-specified*. But whether the mis-specification should lead us to try other models will depend on our specific interests.

Correlations/patterns in simulations/sampling may provide poorer inference or less information on the 'true' value? May take longer to converge to correct value.

`Subjective knowledge of an empirical domain provides expertise. Expertise in turn allows for imaginative checks of model performance. Since golems have terrible imaginations, we need the freedom to engage our own imagination. In this way, the objective and subjective work together.` 

## Chapter 4 Linear models

Linear regression is a descriptive model that corresponds to many different process models. Not universally useful, but a strong claim to being foundational. Once you learn to build and interpret linear regression models, you can quickly move on to other types of regressions which are less normal

### 4.1 Why are normal distributions normal

#### 4.1.1 Normal by addition

```{r}
# R code 4.1
# Simulating coin flipping. Heads/tails -> +1/-1 step from the centre of a football pitch. 

# Simulate 1000 random variables. 16 steps. Final position looks normally distributed.
pos <- replicate(1000 , sum( runif(16, -1, 1)))
hist(pos)
plot(density(pos))
```

*Any process that adds together random values from the same distribution converges to normal*

Conceptually, whatever the average calue of the source distribution, each sample can be thought of as a fluctuation from that average value. When we add these values together, they start to cancel each other out. A large positive fluctuation will cancel a large negative one. The more we sum, the more chances for each deviaion from this mean will be canclled out. 

This holds for any distribution (in general). *Technically*, the distribution of sums converges to normal when the original distribution has finite variance. Practically, this means the magnitude of a newly sampled value cannot be so big as to overwhelm the previous values. 

#### 4.1.2. Normal by multiplication

```{r}
# R code 4.2
# Taking the product of growth rates
prod(1 + runif(12, 0, 0.1))
```


```{r}
# R code 4.3
# Repeat this 10,000 times
growth <- replicate( 10000 , prod( 1 + runif(12,0,0.1) ) )
dens( growth , norm.comp=TRUE ) # Approximately normal
```


Multiplying small numbers is approximately the same as addition

$$
1.1\times1.1 = 1.21 \\
1.1\times1.1 = (1 + 0.1)(1 + 0.1) = 1 + 0.2 + 0.01 \approx 1.2
$$

THe smaller the growth rate, the better the additive approximation will be. Small effects that multiply together are approximately additive,so they tend to stabilise on Gaussian distributions.

```{r}
# R code 4.4
# Small is a better approximation of the normal distribution
big <- replicate( 10000 , prod( 1 + runif(12,0,0.5) ) )
small <- replicate( 10000 , prod( 1 + runif(12,0,0.01) ) )
dens(big , norm.comp=TRUE)
dens(small , norm.comp=TRUE)
```

#### 4.1.3. Normal by log-multiplication
Large deviates that are multiplied together tend to produce Gaussian distributions on the log scale.

```{r}
# R code 4.5
log.big <- replicate(10000, log(prod(1 + runif(12, 0, 0.5))))
dens(log.big, norm.comp = TRUE)
```

#### 4.1.4. Using Gaussian distributions - Ontological and Epistemological justification

##### 4.1.4.1 Ontological justification

Ontological - Knowledge that we observe
Epistemological - Knowledge that we build/create

### 4.3 A Gaussian Model of Height

```{r}
# R code 4.7
library(rethinking)
data(Howell1)
d <- Howell1

# 4.8
str(d)
```

```{r}
# R code 4.10
# Work wit adults first. Height correlated with age before adulthood.
d2 <- d[d$age >= 18,]
```

##### 4.3.2. The model

```{r}
# Plot distribution of heights
dens(d2$height) # 'Somewhat' gaussian in shape. Height is the sum of many growth factors. Distribution of sums tend to converge to gaussians.
```
Becareful when choosing a model's likelihood only when the plotted outcome looks like a particular distribution. A gaussian for example may be a mixture of different gaussian distributions. Might be unable to detect the underlying 'normality' or distribution based on the outcome observed. Justification for gaussian likelihood does not require an empirical distribution to be gaussian.

```{r}
# R code 4.11
curve(dnorm(x, 178, 20), from = 100, to = 250)

# R code 4.12
curve(dnorm(x, 0, 50), from = -10, to = 60)
```
The prior choice of $\mu_\sigma$ is the belief with 95% probability that the mean lies within 2 standard deviations of the mean.
The prior choice of $\sigma$ represents your belief of 95% of observations being within two standard deviations of the mean. 
```{r}
# R code 4.13
sample_mu <- rnorm(1e4, 178, 20) # Moves the centre of the density, wider variation effects the peakedness/tailedness (kurtosis). Higher variation here reduces the height of the mode of the density, putting more weight on the tails
sample_sigma <- runif(1e4, 0, 50) # Sigma=50 implies 95% of individual heights lie within 100 cm of average height. 
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h) # Distribution of relative pausibilities of different heights before seeing data.

# Mess around with the priors
sample_mu <- rnorm(1e4, 178, 90) # Higher variation in sigma reduces height + fattness of the tails. 
sample_sigma <- runif(1e4, 0, 50) # Sigma=50 implies 95% of individual heights lie within 100 cm of average height.
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h) # Distribution of relative pausibilities of different heights before seeing data.

```

#### 4.3.3. Grid Approximiation of the posterior distribution

```{r}
# R code 4.14
mu.list <- seq( from=140, to=160 , length.out=200 )
sigma.list <- seq( from=4 , to=9 , length.out=200 )
post <- expand.grid( mu=mu.list , sigma=sigma.list )
post$LL <- sapply( 1:nrow(post) , function(i) sum( dnorm( # Calculate log likelihood for each grid value. Sum log densities
    d2$height ,
    mean=post$mu[i] ,
    sd=post$sigma[i] ,
    log=TRUE ) ) )
post$prod <- post$LL + dnorm( post$mu , 178 , 20 , TRUE ) +
dunif( post$sigma , 0 , 50 , TRUE )
post$prob <- exp( post$prod - max(post$prod) ) # Scaled by max. Relative probabilities. Taking exp(post$prod) will round to zero.
```

```{r}
# R code 4.15, 4.16
contour_xyz(post$mu, post$sigma, post$prob)
image_xyz(post$mu, post$sigma, post$prob)
```

```{r}
# R code 4.17
sample.rows <- sample( 1:nrow(post) , size=1e4 , replace=TRUE ,
prob=post$prob )
sample.mu <- post$mu[ sample.rows ]
sample.sigma <- post$sigma[ sample.rows ]

# R code 4.18
plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )
```

```{r}
# R code 4.19
# Marginal posterior densities of my and sigma. Marginal ~ Averaging over other parameters
dens(sample.mu)
dens(sample.sigma)
```

```{r}
# R code 4.20
HPDI(sample.mu)
HPDI(sample.sigma)
```

```{r}
# R code 4.21
d3 <- sample(d2$height, size = 20)
```

```{r}
# R code 4.22
# Repeating grid approxmiation of posterior with sample size of 20. 
mu.list <- seq( from=150, to=170 , length.out=200 )
sigma.list <- seq( from=4 , to=20 , length.out=200 )
post2 <- expand.grid( mu=mu.list , sigma=sigma.list )
post2$LL <- sapply( 1:nrow(post2) , function(i)
sum( dnorm( d3 , mean=post2$mu[i] , sd=post2$sigma[i] ,
log=TRUE ) ) )
post2$prod <- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) +
dunif( post2$sigma , 0 , 50 , TRUE )
post2$prob <- exp( post2$prod - max(post2$prod) )
sample2.rows <- sample( 1:nrow(post2) , size=1e4 , replace=TRUE ,
prob=post2$prob )
sample2.mu <- post2$mu[ sample2.rows ]
sample2.sigma <- post2$sigma[ sample2.rows ]
plot( sample2.mu , sample2.sigma , cex=0.5 ,
col=col.alpha(rangi2,0.1) ,
xlab="mu" , ylab="sigma" , pch=16 )
```

```{r}
# R code 4.23
dens(sample2.sigma , norm.comp=TRUE )
```

```{r}
# R code 4.24
library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18,]
```

```{r}
# R code 4.25
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)
```

```{r}
# R code 4.26
m4.1 <- map( flist , data=d2 )
```

```{r}
# R code 4.27
precis(m4.1)
```

```{r}
# R code 4.28
# Choosing initial values. List executes code embedded inside, alist does not (simply writes out the formula) 
start <- list(
  mu = mean(d2$height),
  sigma = sd(d2$height)
)
```

```{r}
# R code 4.29
m4.2 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(178, 0.1), # Narrower prior, more confident on location of the mean. 
    sigma ~ dunif(0, 50)
  ),
  data = d2
)
precis(m4.2)
```

Created certainty in the model that the mean is near 178 as defined by the prior. Sigma, conditional on this, is much larger. 

A prior can be thought of as former posterior information. It can be useful to talk about the strgenth of a prior in terms of which data would lead to the same posterior distribution. We can compute the implied amount of data from the standard deviation of a Gaussian posterior for $\mu$:

$$
\sigma_{post} = 1 / \sqrt{n}
$$

$\mu\sim Normal(178, 0.1)$ implies $n = 1/0.01 = 100$ observations, this is a strong prior. 
$\mu\sim Normal(178, 20)$ implies $n = 1/20^2 = 0.0025$ of an observation, this is a weak prior.

#### 4.3.6 Sampling from a map fit
Sampling from a posterior distribution gives combinations of parameter values PLUS their covariances.

```{r}
# R code 4.30
vcov(m4.1)
```

```{r}
# R code 4.31
diag(vcov(m4.1))
cov2cor(vcov(m4.1))
```


```{r}
# R code 4.32
library(rethinking)
post <- extract.samples(m4.1, n=1e4)
head(post)
```

```{r}
# R code 4.33
precis(post)
```

```{r}
# R code 4.34
# Simulating from multivariate gaussians
library(MASS)
post <- mvrnorm(n=1e4, mu = coef(m4.1), Sigma=vcov(m4.1))
```

Quadratic assumption for $\sigma$ is problamatic (only positive, positively skewed). Taking the log of this distribution often results in a gaussian which leads to a better quadratic approximation.

```{r}
# R code 4.35
m4.1_logsigma <- map(
  alist(
    height ~ dnorm(mu, exp(log_sigma)),
    mu ~ dnorm(178, 20),
    log_sigma ~ dnorm(2, 10) # Now have a gaussian prior
  ), data = d2
)
```

```{r}
# R code 4.36
# Getting back the distribution of sigma
post <- extract.samples(m4.1_logsigma)
sigma <- exp(post$log_sigma)
```

### 4.4 Adding a predictor

```{r}
# R code 4.37
# Knowing a persons weight helps predict height
plot(d2$height ~ d2$weight)
```
Regression ~ Regressing to the mean. When individual measurements are assigned a common distribution, this leads to *shrinkage* as each measurement informs the others. 

#### 4.4.1 The linear model strategy

The linear model strategy assumes that the predictor variable has a perfectly constant and additive relationship to the mean of the outcome. 

The parameteres of a linear model stand for the strength of association between the mean of the outcome and the value of the predictor. For each comnination of values, the machine computes the posterior probability (a measure of relative plausibility), given the model and data. The posterior distribution gives relative plausibilities of the different possible strengths of association, given the assumptions programmed in the model. 

$$
h_i\sim N(\mu_i,\sigma)\\
\mu_i = \alpha + \beta x_i \\
\alpha \sim N(178,100) \\
\beta\sim N(0,10) \\
\sigma\sim U(0,50)\\
$$
Note that $\mu_i$ is now deterministic, and not stochastic. $\alpha$ and $\beta$ are devices invented for manipulating $\mu$. They are targets of learning, something that must be described in the posterior density. 

Aside: Units. Note here the $x_i$ is in kg and height is in cm. The kgs cancel out so that $\mu_i$ has units in cm. $\beta$ can be thought of some kind of weight, the number of cm per kg. 

$$
h_icm\sim N(\mu_i cm,\sigma cm)\\
\mu_i cm = \alpha cm + \beta \frac{cm}{kg} x_i kg \\

$$

$\alpha = \mu$ before introducting predictor variable. The prior for $\alpha$ is widnened, as it is common for the intercept of a linear model to swing a long way from the mean of the outcome variable. 

$\beta$ with mean 0 puts most of the prior probability on zero. This is seen as a conservative assumption as it pulls the probability mass towards zero. *But a gaussian prior with a standard deviation of 10 is still very weak, so the amount of conservatism it induces will be small.* This induces a lot variation in parameter values away from zero. As you make the SD smaller, the model produces more conservative esimates around zero, and therefore more conservative esitmate about the relationship between height and weight. 

#### 4.4.2 Fitting the model

```{r}
# R code 4.38
# load data again, since it's a long way back
library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[ d$age >= 18 , ]
# fit model
m4.3 <- map(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b*weight , # mu not a parameter here. Replaced by linear model.
    a ~ dnorm( 156 , 100 ) ,
    b ~ dnorm( 0 , 10 ) ,
    sigma ~ dunif( 0 , 50 )
) ,
data=d2 )
```

*Everything that depends upon parameters has a posterior distribution.* $\alpha$ and $\beta$ have a joint distribution, therefore so does $mu$ (even though it's not a parameter anymore). Since parameters are uncertain, everything that depnds upon them is also uncertain. 

```{r}
# R code 4.39 (CAUSES ERRORS. doesn't add the intercept. link(m4.3) to check. Use 4.38)
# m4.3 <- map(
#   alist(
#     height ~ dnorm( a + b*weight , sigma ) ,
#     a ~ dnorm( 178 , 100 ) ,
#     b ~ dnorm( 0 , 10 ) ,
#     sigma ~ dunif( 0 , 50 )
# ) ,
# data=d2 )
```

#### 4.4.3. Interpreting model fit
```{r}
# R code 4.40
precis(m4.3)
```

The estimate of $\alpha$ tells us the value of $\mu$ when the predictor variables have value zero. As a consequence, the value of the intercept is frequently uninterpretable without also studying any $\beta$ parameters. This is why we need very weak priors for intercepts in many cases. 

$\sigma$ informs the width of the distribution of heights around the mean. 95% of probability in a gaussian distribution lies between two standard deviations. So the results tell us 95% of plausible heights lie within 10cm ($2\sigma$) of the mean height. The uncertainty of this estimate is indicated by the 89% percentile interval.

```{r}
# R code 4.4.0
precis(m4.3, corr = TRUE)
```

$\alpha$ and $\beta$ are almost perfectly negatively correlated. It is harmless for now, it means that these two parameters carry the same information -- as you change the slope of the line, the best intercept changes to match it. 

In more complex models, correlation between parameters make it difficult to fit the model to the data. So we can employ some tricks to avoid it when possible.

The first trick is *centering*, this procedure subtracts the mean of a variable from each value. This results in the transformed variable having mean zero.

```{r}
# R code 4.42
d2$weight.c <- d2$weight - mean(d2$weight)
```

```{r}
# R code 4.43
m4.4 <- map(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b*weight.c ,
    a ~ dnorm( 178 , 100 ) ,
    b ~ dnorm( 0 , 10 ) ,
    sigma ~ dunif( 0 , 50 )
) ,
data=d2 )
```

```{r}
# R code 4.44
precis(m4.4, corr = TRUE)
```

$\alpha$ now the same as average height value in the raw data. Correlations between intercept and slope now zero.

The estimate for the incercept is the expected value of the outcome variable when the predictor variable is zero. The *mean* value of the predictor is now zero. Therefore, the incercept also means: the expected value of the outcome, when the predictor is at its average value (since alpha is mean when x = 0 = x_bar).

**Plotting posterior inference against the data.**

```{r}
# R code 4.45
plot(height ~ weight, data = d2)
abline(a = coef(m4.3)["a"], b = coef(m4.3)["b"])
```

##### Adding uncertainty around the mean (unceratinty around fitted mu, the linear model)

The MAP line is just the posterior mean, the most plausible line out of all the lines the posterior has considered. Does not clearly communicate uncertainty.

The posterior distribution considers every possible regression line connecting height and weight, assigning a relative plausibility to each. 

```{r}
# R code 4.46
post <- extract.samples(m4.3)
```

```{r}
# R code 4.47
# Each row is a correlated random sample from the joint posterior of all three parameters, using covariances provided by vcov(m4.3)
post[1:5,]
#vcov(m4.3)
```

```{r}
# R code 4.48
# Start with first 10 observations to see how data size influences uncertainty
N <- 352 # Change this (otherwise 4.51 won't look the same in the book)
dN <- d2[ 1:N , ]
mN <- map(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b*weight ,
    a ~ dnorm( 178 , 100 ) ,
    b ~ dnorm( 0 , 10 ) ,
    sigma ~ dunif( 0 , 50 )
) , data=dN )

# R code 4.49
# extract 20 samples from the posterior
post <- extract.samples( mN , n=20000 ) # Change samples here too for 4.41 density
# display raw data and sample size
plot( dN$weight , dN$height ,
  xlim=range(d2$weight) , ylim=range(d2$height) ,
  col=rangi2 , xlab="weight" , ylab="height" )
mtext(concat("N = ",N))
# plot the lines, with transparency
for ( i in 1:20 )
  abline( a=post$a[i] , b=post$b[i] , col=col.alpha("black",0.3) )
```

**Plotting regression intervals and contours**
```{r}
# R code 4.50
mu_at_50 <- post$a + post$b*50
```

```{r}
# R code 4.51
dens(mu_at_50, col = rangi2, lwd = 2, xlab="mu|weight=50") # check 4.48 and 4.49 if things dont look right
```

```{r}
# R code 4.52
HPDI(mu_at_50, prob = 0.89)
```

```{r}
# R code 4.53
# Rows -> Samples from posterior. Columns -> 352 individuals/observaions from the data. 
# So sampling posterior of mu (y_hat or conditional mean) for each value of x_i
mu1 <- link(m4.3)
str(mu1)
```

```{r}
# R code 4.54 
# define sequence of weights to compute predictions for
# these values will be on the horizontal axis
weight.seq <- seq(from = 25, to = 70, by = 1) # 46 new x values

# Use link to compute mu
# for each sample from posterior
# and for each weight in weight.seq
mu <- link(mN, data = data.frame(weight = weight.seq)) # 46 columns sampling mu from
str(mu)
```

```{r}
# R code 4.55
# Use type = "n" to hide raw data
plot(height ~ weight, d2, type = "n")
# Loop over samples and plot each mu alue
for (i in 1:100){
  points(weight.seq, mu[i,], pch = 16, col = rangi2)
}

```
```{r}
# R code 4.56
# Summarise the distribution of mu
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.89)
```

```{r}
# R code 4.57
# plot raw data
# fading out points to make line and interval more visible
plot(height ~ weight, data = d2, col = col.alpha(rangi2, 0.5))

# plot the MAP line, aka the mean mu for each weight
lines(weight.seq, mu.mean)

# plot a shaded region for 89% HPDI
shade(mu.HPDI, weight.seq)
```


```{r}
# R code 4.58
# Under the hood of link, simulating conditional mean/linear model
post <- extract.samples(m4.3)
mu.link <- function(weight) post$a + post$b*weight.c
weight.seq <- seq(from = 25, to = 70, by = 1)
mu <- sapply(weight.seq, mu.link)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, probs = 0.89)
```
##### Prediction intervals (prediction of y, the data)

```{r}
# R code 4.59
# Simulated HEIGHTS not distributions of possible average heights (mu)
sim.height <- sim(m4.3, data = list(weight = weight.seq))
str(sim.height)
```

```{r}
# R code 4.60
# Caclculate 89% prediction interval  of observable (according to the model) heights
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
```
```{r}
# R code 4.61
# Plot of 89% prediction intervals for heights

# Plot raw data
plot( height ~ weight , d2 , col=col.alpha(rangi2,0.5) )
# draw MAP line
lines( weight.seq , mu.mean )
# draw HPDI region for line
shade( mu.HPDI , weight.seq )
# draw PI region for simulated heights
shade( height.PI , weight.seq )
```

Jagged outline for prediction interval due to simulation variance in the tails of the sampled Gaussian values. Increase the number of samples drawn to smoothen this out. 

```{r}
# R code 4.62
sim.height <- sim(m4.3, data = list(weight = weight.seq), n = 1e4)
height.PI <- apply(sim.height,2, PI, prob = 0.89)
```

Encountered two kinds of uncertainty (parameter values and sampling process).  Use of the gaussain likelihood may be a purely epistemoogical assumtion (used to estimate mean/variance of a variable), rather than an ontological assumption about what future data will look like. If so, may not make sense to simulate outcomes. 

```{r}
# R code 4.63 under the hood of the sim function
post <- extract.samples(m4.3)
weight.seq <- 25:70
sim.height <- sapply( weight.seq , function(weight)
  rnorm(
    n=nrow(post) ,
    mean=post$a + post$b*weight ,
    sd=post$sigma ) )
height.PI <- apply( sim.height , 2 , PI , prob=0.89 )
```



### 4.5 Plynomial regression

```{r}
# R code 4.64
library(rethinking)
data(Howell1)
d <- Howell1
str(d)

plot(d$height ~ d$weight) # non linear
```

```{r}
# R code 4.65
# Standardising variables. Interpretation might be easier (one unit is one std dev)
# Easier to fit model to data. Particularly for polynomial regression. 
d$weight.s <- ( d$weight - mean(d$weight) )/sd(d$weight)
```

```{r}
# R code 4.66
d$weight.s2 <- d$weight.s^2
m4.5 <- map(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b1*weight.s + b2*weight.s2 ,
    a ~ dnorm( 178 , 100 ) ,
    b1 ~ dnorm( 0 , 10 ) ,
    b2 ~ dnorm( 0 , 10 ) ,
    sigma ~ dunif( 0 , 50 )
) ,
data=d )

# R code 4.67
precis(m4.5)
```

```{r}
# R code 4.68
weight.seq <- seq( from=-2.2 , to=2 , length.out=30 )
pred_dat <- list( weight.s=weight.seq , weight.s2=weight.seq^2 )
mu <- link( m4.5 , data=pred_dat )
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )
sim.height <- sim( m4.5 , data=pred_dat )
height.PI <- apply( sim.height , 2 , PI , prob=0.89 )
```

```{r}
# R code 4.69
plot( height ~ weight.s , d , col=col.alpha(rangi2,0.5) )
lines( weight.seq , mu.mean )
shade( mu.PI , weight.seq )
shade( height.PI , weight.seq )
```
```{r}
# R code 4.70
d$weight.s3 <- d$weight.s^3
m4.6 <- map(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b1*weight.s + b2*weight.s2 + b3*weight.s3 ,
    a ~ dnorm( 178 , 100 ) ,
    b1 ~ dnorm( 0 , 10 ) ,
    b2 ~ dnorm( 0 , 10 ) ,
    b3 ~ dnorm( 0 , 10 ) ,
    sigma ~ dunif( 0 , 50 )
) ,
data=d )
```

```{r}
# R code 4.71
# Converting to natural scale. Turn off horizontal axis
plot( height ~ weight.s , d , col=col.alpha(rangi2,0.5) , xaxt="n" )

# R code 4.72 Construct the axis manually
at <- c(-2,-1,0,1,2)
labels <- at*sd(d$weight) + mean(d$weight)
axis( side=1 , at=at , labels=round(labels,1) )

```


## Chapter 5 Multivariate Linear Models
Reasons for using mulivariate Regressions:

1) Statistical "control" for confounds. Where confounds are variables that may be correlated with another variable of interest. 
2) Multiple causation. In the absense of confounds (due to perhaps, tight experimental design), we can use multivariate regression to estimate multiple causes. Some causes may 'mask' the effect of others, so it is important to capture these effects.
3) Interactions. Where effectiveness about one variable depends on consideration of other variables (e.g. water and sunlight for plant growth. Plant can't grow with only one of these.)

### 5.1 Spurious Association
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# R code 5.1 
library(rethinking)
data(WaffleDivorce)
d <- WaffleDivorce

# Standardise the predictor
d$MedianAgeMarriage.s <- (d$MedianAgeMarriage - mean(d$MedianAgeMarriage)) / sd(d$MedianAgeMarriage)

# Fit model
m5.1 <- map(
  alist(
  Divorce ~ dnorm(mu, sigma),
  mu <- a + bA * MedianAgeMarriage.s,
  a ~ dnorm(10, 10),
  bA ~ dnorm(0, 1),
  sigma ~ dunif(0, 10)
  ),
data = d
)
```

```{r}
# R code 5.2

# Compute percentile interval of mean
MAM.seq <- seq(from = -3, to = 3.5, length.out = 30)
mu <- link(m5.1, data = data.frame(MedianAgeMarriage.s = MAM.seq))
mu.PI <- apply(mu, 2, PI)

# Plot
plot(Divorce ~ MedianAgeMarriage.s, data = d, col = rangi2)
abline(m5.1)
shade(mu.PI, MAM.seq)

precis(m5.1)
```

```{r}
# R code 5.3
d$Marriage.s <- (d$Marriage - mean(d$Marriage)) / sd(d$Marriage)

m5.2 <- map(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu <- a + bR * Marriage.s,
    a ~ dnorm(10, 10),
    bR ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ) , data = d
)

precis(m5.2)
```

```{r}
# R code 5.4
m5.3 <- map(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu <- a + bR*Marriage.s + bA*MedianAgeMarriage.s,
    a ~ dnorm(10, 10),
    bR ~ dnorm(0, 1),
    bA ~ dnorm(0 , 1),
    sigma ~ dunif(0, 10)
  ) , data = d
)
precis(m5.3) # R code 5.5
```


#### 5.1.3. Plotting mltivariace posteriors

1) Predictor residual plots
2) Counter factual plots
3) Posterior prediction plots

*Predictor residual plots*
Partialing out relationshop of other predictors and plotting residuals against the dependent variable
```{r}
# R code 5.6
m5.4 <- map(
  alist(
    Marriage.s ~ dnorm(mu, sigma),
    mu <- a + b*MedianAgeMarriage.s,
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), data = d
)

# R code 5.7 
# Compute residuals
mu <- coef(m5.4)['a'] + coef(m5.4)['b']*d$MedianAgeMarriage.s
m.resid <- d$Marriage.s- mu

# R code 5.8
plot(Marriage.s ~ MedianAgeMarriage.s, d, col = rangi2)
abline(m5.4)

# Loop over States
for(i in 1:length(m.resid)){
  x <- d$MedianAgeMarriage.s[i] # x location of line segment
  y <- d$Marriage.s[i] # observed endpoint of line segment
  lines(c(x,x), c(mu[i],y), lwd=0.5, col=col.alpha("black",0.7))
}
```

*5.1.3.2. Counter factual plots*
Plotting relationship between dependent and predictor, holding all else constant.

```{r}
# R code 5.9
# Prepare new counterfactual data
A.avg <- mean(d$MedianAgeMarriage.s)
R.seq <- seq(from = -3, to = 3, length.out = 30)

pred.data <- data.frame(
  Marriage.s = R.seq,
  MedianAgeMarriage.s = A.avg
)

# Compute counterfactual mean divorce (mu)
mu <- link(m5.3, data = pred.data)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

# Simulate counterfactual divorce outcomes
R.sim <- sim(m5.3, data = pred.data, n = 1e4)

R.PI <- apply(R.sim, 2, PI)

# Display predictions
plot(Divorce ~ Marriage.s, data = d, type = "n")
mtext("MedianAgeMarriage.s = 0")
lines(R.seq, mu.mean)
shade(mu.PI, R.seq)
shade(R.PI, R.seq)
```

```{r}
## R code 5.10
R.avg <- mean( d$Marriage.s ) # Could be left out since centred on 0
A.seq <- seq( from=-3 , to=3.5 , length.out=30 )
pred.data2 <- data.frame(
    Marriage.s=R.avg,
    MedianAgeMarriage.s=A.seq
)

mu <- link( m5.3 , data=pred.data2 )
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI )

A.sim <- sim( m5.3 , data=pred.data2 , n=1e4 )
A.PI <- apply( A.sim , 2 , PI )

plot( Divorce ~ MedianAgeMarriage.s , data=d , type="n" )
mtext( "Marriage.s = 0" )
lines( A.seq , mu.mean )
shade( mu.PI , A.seq )
shade( A.PI , A.seq )
```

*5.1.3.3. Posterior Prediction plots*
Checking model fit against observed plots. Two questions:

1) Did the model fit correctly? (Human or software error)
2) How does the model fail? (Not all models are perfect)

```{r}
## R code 5.11
# call link without specifying new data
# so it uses original data
mu <- link( m5.3 )

# summarize samples across cases
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI )

# simulate observations
# again no new data, so uses original data
divorce.sim <- sim( m5.3 , n=1e4 )
divorce.PI <- apply( divorce.sim , 2 , PI )
```

```{r}
## R code 5.12
plot( mu.mean ~ d$Divorce , col=rangi2 , ylim=range(mu.PI) ,
    xlab="Observed divorce" , ylab="Predicted divorce" )
abline( a=0 , b=1 , lty=2 )
for ( i in 1:nrow(d) )
    lines( rep(d$Divorce[i],2) , c(mu.PI[1,i],mu.PI[2,i]) ,
        col=rangi2 )
## R code 5.13
identify( x=d$Divorce , y=mu.mean , labels=d$Loc , cex=0.8 )
```

```{r}
## R code 5.14
# compute residuals
divorce.resid <- d$Divorce - mu.mean
# get ordering by divorce rate
o <- order(divorce.resid)
# make the plot
dotchart( divorce.resid[o] , labels=d$Loc[o] , xlim=c(-6,5) , cex=0.6 )
abline( v=0 , col=col.alpha("black",0.2) )
for ( i in 1:nrow(d) ) {
    j <- o[i] # which State in order
    lines( d$Divorce[j]-c(mu.PI[1,j],mu.PI[2,j]) , rep(i,2) )
    points( d$Divorce[j]-c(divorce.PI[1,j],divorce.PI[2,j]) , rep(i,2),
        pch=3 , cex=0.6 , col="gray" )
}
```

We can also use the resduals in models with othe predictors to see if other predictors have an association with the dependent variable once the other predictors have been partialled out (or after controlling for the other variables). Note that it is always possible to find spurious correlations with residuals and other predictors. 

```{r}
# R code 5.15
N <- 100 # number of cases
x_real <- rnorm( N ) # x_real as Gaussian with mean 0 and stddev 1
x_spur <- rnorm( N , x_real ) # x_spur as Gaussian with mean=x_real
y <- rnorm( N , x_real ) # y as Gaussian with mean=x_real
d <- data.frame(y,x_real,x_spur) # bind all together in data frame
pairs(d)
```


### 5.2 Masked relationship
```{r}
## R code 5.16
library(rethinking)
data(milk)
d <- milk
str(d)
```

```{r}
## R code 5.17
m5.5 <- map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a + bn*neocortex.perc ,
        a ~ dnorm( 0 , 100 ) ,
        bn ~ dnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 1 )
    ) ,
    data=d )
```
```{r}
## R code 5.18
d$neocortex.perc

## R code 5.19
dcc <- d[ complete.cases(d) , ]
```

```{r}
## R code 5.20
m5.5 <- map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a + bn*neocortex.perc ,
        a ~ dnorm( 0 , 100 ) ,
        bn ~ dnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 1 )
    ) ,
    data=dcc )

## R code 5.21
precis( m5.5 , digits=3 )
```

```{r}
## R code 5.22
coef(m5.5)["bn"] * ( 76 - 55 )
```

```{r}
## R code 5.23
np.seq <- 0:100
pred.data <- data.frame( neocortex.perc=np.seq )

mu <- link( m5.5 , data=pred.data , n=1e4 )
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI )

plot( kcal.per.g ~ neocortex.perc , data=dcc , col=rangi2 )
lines( np.seq , mu.mean )
lines( np.seq , mu.PI[1,] , lty=2 )
lines( np.seq , mu.PI[2,] , lty=2 )
```

Taking the log of a measure translates the measure into magnitudes

```{r}
## R code 5.24
dcc$log.mass <- log(dcc$mass)

## R code 5.25
m5.6 <- map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a + bm*log.mass ,
        a ~ dnorm( 0 , 100 ) ,
        bm ~ dnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 1 )
    ) ,
    data=dcc )
precis(m5.6)
```

```{r}
## R code 5.26
m5.7 <- map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a + bn*neocortex.perc + bm*log.mass ,
        a ~ dnorm( 0 , 100 ) ,
        bn ~ dnorm( 0 , 1 ) ,
        bm ~ dnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 1 )
    ) ,
    data=dcc )
precis(m5.7)
```

```{r}
## R code 5.27
mean.log.mass <- mean( log(dcc$mass) )
np.seq <- 0:100
pred.data <- data.frame(
    neocortex.perc=np.seq,
    log.mass=mean.log.mass
)

mu <- link( m5.7 , data=pred.data , n=1e4 )
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI )

plot( kcal.per.g ~ neocortex.perc , data=dcc , type="n" )
lines( np.seq , mu.mean )
lines( np.seq , mu.PI[1,] , lty=2 )
lines( np.seq , mu.PI[2,] , lty=2 )

# Try with log mass
mean.neocortex.perc <- mean( dcc$neocortex.perc)
np.seq <- -10:100
pred.data <- data.frame(
    neocortex.perc=mean.neocortex.perc,
    log.mass=np.seq
)

mu <- link( m5.7 , data=pred.data , n=1e4 )
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI )

plot( kcal.per.g ~ log.mass , data=dcc , type="n" )
lines( np.seq , mu.mean )
lines( np.seq , mu.PI[1,] , lty=2 )
lines( np.seq , mu.PI[2,] , lty=2 )

```
Mass negatively correlated with outcome, neocortex positively correlated with outcome. Both predictors *positively* correlated with each other. This understates (biases) the relationship each has with the outcome. Omitting neocortex creates a positive bias in mass/outcome, omitting mass creates a negative bias in neocortex/outcome.

Model asks if species with high neocortex percent *for their body mass (given)* have higher milk energy and whether high mass *given their neocortex percentage* have migher milk energy. 
```{r}
## R code 5.28
N <- 100                         # number of cases
rho <- 0.7                       # correlation btw x_pos and x_neg
x_pos <- rnorm( N )              # x_pos as Gaussian
x_neg <- rnorm( N , rho*x_pos ,  # x_neg correlated with x_pos
    sqrt(1-rho^2) )
y <- rnorm( N , x_pos - x_neg )  # y equally associated with x_pos, x_neg
d <- data.frame(y,x_pos,x_neg)   # bind all together in data frame
pairs(d)
```
### 5.3 When adding variables hurts

#### Multicollinearity
```{r}
## R code 5.29
N <- 100                          # number of individuals
height <- rnorm(N,10,2)           # sim total height of each
leg_prop <- runif(N,0.4,0.5)      # leg as proportion of height
leg_left <- leg_prop*height +     # sim left leg as proportion + error
    rnorm( N , 0 , 0.02 )
leg_right <- leg_prop*height +    # sim right leg as proportion + error
    rnorm( N , 0 , 0.02 )
                                  # combine into data frame
d <- data.frame(height,leg_left,leg_right)

## R code 5.30
m5.8 <- map(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + bl*leg_left + br*leg_right ,
        a ~ dnorm( 10 , 100 ) ,
        bl ~ dnorm( 2 , 10 ) ,
        br ~ dnorm( 2 , 10 ) ,
        sigma ~ dunif( 0 , 10 )
    ) ,
    data=d )
precis(m5.8)
```

```{r}
## R code 5.31
plot(precis(m5.8))
```
```{r}
## R code 5.32
post <- extract.samples(m5.8)
plot( bl ~ br , post , col=col.alpha(rangi2,0.1) , pch=16 )
```

Highly correlated posteriors for left and right leg parameters. When one is large, the other most be small, cancelling each other out since both variables contain almost exactly the same information. There are an infinite number of combinations that produce the same predictions.

```{r}
## R code 5.33
sum_blbr <- post$bl + post$br
dens( sum_blbr , col=rangi2 , lwd=2 , xlab="sum of bl and br" )
```

```{r}
## R code 5.34
## Using one leg gives close to same posterior mean has the sum of both bl and br
m5.9 <- map(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + bl*leg_left,
        a ~ dnorm( 10 , 100 ) ,
        bl ~ dnorm( 2 , 10 ) ,
        sigma ~ dunif( 0 , 10 )
    ) ,
    data=d )
precis(m5.9)

```

#### 5.3.2. Multicollinear milk
```{r}
## R code 5.35
library(rethinking)
data(milk)
d <- milk
```

```{r}
## R code 5.36
# kcal.per.g regressed on perc.fat
m5.10 <- map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a + bf*perc.fat ,
        a ~ dnorm( 0.6 , 10 ) ,
        bf ~ dnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 10 )
    ) ,
    data=d )

# kcal.per.g regressed on perc.lactose
m5.11 <- map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a + bl*perc.lactose ,
        a ~ dnorm( 0.6 , 10 ) ,
        bl ~ dnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 10 )
    ) ,
    data=d )

precis( m5.10 , digits=3 )
precis( m5.11 , digits=3 )
```

```{r}
## R code 5.37
# SDs twice as large, post means close to 0. Contain same information, describing a 
# long ridge of combinations of bf and bl that are equally possible.
m5.12 <- map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a + bf*perc.fat + bl*perc.lactose ,
        a ~ dnorm( 0.6 , 10 ) ,
        bf ~ dnorm( 0 , 1 ) ,
        bl ~ dnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 10 )
    ) ,
    data=d )
precis( m5.12 , digits=3 )

```

```{r}
## R code 5.38 
## These two variables form essentially form a single axis of variation
## Two explanatory variables are close to perfectly negatively correlated
## individually, they help oredict the dependent, but *neither helps once
## you already know the other*.
pairs( ~ kcal.per.g + perc.fat + perc.lactose ,
    data=d , col=rangi2 )
```

```{r}
## R code 5.39
cor( d$perc.fat , d$perc.lactose )
```

*Check vovariation among parameters, especially when intervals are extremely wide, so you dont' misinterpret what the model is trying to tell you*

As long as porterior integrates to 1, then all parameters are identified. But sometimes with wide intervals it's hard to make sense of the post dist. So we can say it's "weakly identified".

```{r}
## R code 5.40
## Simulating correlated variables.
library(rethinking)
data(milk)
d <- milk
sim.coll <- function( r=0.9 ) {
    d$x <- rnorm( nrow(d) , mean=r*d$perc.fat ,
        sd=sqrt( (1-r^2)*var(d$perc.fat) ) )
    m <- lm( kcal.per.g ~ perc.fat + x , data=d )
    sqrt( diag( vcov(m) ) )[2] # stddev of parameter
}
rep.sim.coll <- function( r=0.9 , n=100 ) {
    stddev <- replicate( n , sim.coll(r) )
    mean(stddev)
}
r.seq <- seq(from=0,to=0.99,by=0.01)
stddev <- sapply( r.seq , function(z) rep.sim.coll(r=z,n=100) )
plot( stddev ~ r.seq , type="l" , col=rangi2, lwd=2 , xlab="correlation" )
```

```{r}
## R code 5.41
# number of plants
N <- 100

# simulate initial heights
h0 <- rnorm(N,10,2)

# assign treatments and simulate fungus and growth
treatment <- rep( 0:1 , each=N/2 )
fungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 )
h1 <- h0 + rnorm(N, 5 - 3*fungus)

# compose a clean data frame
d <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus )

```

```{r}
## R code 5.42
## Including fungus, a post treatment variable. Post mean of treatment does not make sense
m5.13 <- map(
    alist(
        h1 ~ dnorm(mu,sigma),
        mu <- a + bh*h0 + bt*treatment + bf*fungus,
        a ~ dnorm(0,100),
        c(bh,bt,bf) ~ dnorm(0,10),
        sigma ~ dunif(0,10)
    ),
    data=d )
precis(m5.13)
```

```{r}
## R code 5.43
## Excluding post treatment variable.
m5.14 <- map(
    alist(
        h1 ~ dnorm(mu,sigma),
        mu <- a + bh*h0 + bt*treatment,
        a ~ dnorm(0,100),
        c(bh,bt) ~ dnorm(0,10),
        sigma ~ dunif(0,10)
    ),
    data=d )
precis(m5.14)
```

Pre treatment vaeriables may mask causal inference of treatment *if excluded from the model*, so we should include them in the model.

Post treatment varaiables may mask the treatment itself if *included in the model*, so we should exclude them from the model.

Note the model may give better predictions if we include the post treatment variable, it may fit the sample better. But if we care about causal inference, such models are answering the wrong question. Hence we cannot use information criteria for comparison of model validity in this case. It is measuring the wrong metric.

### 5.4 Categorical variables

```{r}
## R code 5.44
data(Howell1)
d <- Howell1
str(d)

## R code 5.45
m5.15 <- map(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + bm*male ,
        a ~ dnorm( 178 , 100 ) ,
        bm ~ dnorm( 0 , 10 ) ,
        sigma ~ dunif( 0 , 50 )
    ) ,
    data=d )
precis(m5.15)

```

To get average male height, add the samples of alpha and beta together to get the posterior distribution of their sum. Can't just add boundaries together (quantiles), since the parameteres are correlated. Dealing with samples is one way around this problem.

```{r}
## R code 5.46
post <- extract.samples(m5.15)
mu.male <- post$a + post$bm
PI(mu.male)
```

```{r}
## R code 5.47
m5.15b <- map(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- af*(1-male) + am*male ,
        af ~ dnorm( 178 , 100 ) ,
        am ~ dnorm( 178 , 100 ) ,
        sigma ~ dunif( 0 , 50 )
    ) ,
    data=d )

```

```{r}
## R code 5.48
data(milk)
d <- milk
unique(d$clade)

## R code 5.49
( d$clade.NWM <- ifelse( d$clade=="New World Monkey" , 1 , 0 ) )

## R code 5.50
d$clade.OWM <- ifelse( d$clade=="Old World Monkey" , 1 , 0 )
d$clade.S <- ifelse( d$clade=="Strepsirrhine" , 1 , 0 )

## R code 5.51
m5.16 <- map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a + b.NWM*clade.NWM + b.OWM*clade.OWM + b.S*clade.S ,
        a ~ dnorm( 0.6 , 10 ) ,
        b.NWM ~ dnorm( 0 , 1 ) ,
        b.OWM ~ dnorm( 0 , 1 ) ,
        b.S ~ dnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 10 )
    ) ,
    data=d )
precis(m5.16)

```

```{r}
## R code 5.52
# sample posterior
post <- extract.samples(m5.16)

# compute averages for each category
mu.ape <- post$a
mu.NWM <- post$a + post$b.NWM
mu.OWM <- post$a + post$b.OWM
mu.S <- post$a + post$b.S

# summarize using precis
precis( data.frame(mu.ape,mu.NWM,mu.OWM,mu.S) )

## R code 5.53
diff.NWM.OWM <- mu.NWM - mu.OWM
quantile( diff.NWM.OWM , probs=c(0.025,0.5,0.975) )
```

```{r}
## R code 5.54
( d$clade_id <- coerce_index(d$clade) )

## R code 5.55
m5.16_alt <- map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a[clade_id] ,
        a[clade_id] ~ dnorm( 0.6 , 10 ) ,
        sigma ~ dunif( 0 , 10 )
    ) ,
    data=d )
precis( m5.16_alt , depth=2 )
```

### 5.5 OLS and LM

```{r}
## R code 5.56
m5.17 <- lm( y ~ 1 + x , data=d )
m5.18 <- lm( y ~ 1 + x + z + w , data=d )

## R code 5.57
m5.17 <- lm( y ~ 1 + x , data=d )
m5.19 <- lm( y ~ x , data=d )

## R code 5.58
m5.20 <- lm( y ~ 0 + x , data=d )
m5.21 <- lm( y ~ x - 1 , data=d )

## R code 5.59
m5.22 <- lm( y ~ 1 + as.factor(season) , data=d )

## R code 5.60
d$x2 <- d$x^2
d$x3 <- d$x^3
m5.23 <- lm( y ~ 1 + x + x2 + x3 , data=d )

## R code 5.61
m5.24 <- lm( y ~ 1 + x + I(x^2) + I(x^3) , data=d )

## R code 5.62
data(cars)
glimmer( dist ~ speed , data=cars )

```

## Chapter 6 Overfitting, Regularisation and Information Criteria

Simplicity vs accuracy. If accuracy is constatnt between models, occam's razor, use the one with the least assumptions (hardly ever the case)

```{r}
## R code 6.1
sppnames <- c( "afarensis","africanus","habilis","boisei",
    "rudolfensis","ergaster","sapiens")
brainvolcc <- c( 438 , 452 , 612, 521, 752, 871, 1350 )
masskg <- c( 37.0 , 35.5 , 34.5 , 41.5 , 55.5 , 61.0 , 53.5 )
d <- data.frame( species=sppnames , brain=brainvolcc , mass=masskg )
```



```{r}
## R code 6.2
m6.1 <- lm( brain ~ mass , data=d )

## R code 6.3
1 - var(resid(m6.1))/var(d$brain)
```

```{r}
## R code 6.4
m6.2 <- lm( brain ~ mass + I(mass^2) , data=d )

## R code 6.5
m6.3 <- lm( brain ~ mass + I(mass^2) + I(mass^3) , data=d )
m6.4 <- lm( brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) ,
    data=d )
m6.5 <- lm( brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) +
    I(mass^5) , data=d )
m6.6 <- lm( brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) +
    I(mass^5) + I(mass^6) , data=d )

```

```{r}
## R code 6.6
m6.7 <- lm( brain ~ 1 , data=d )
```

Underfit/overfit models (high biased/high variance): the difference is the sensitivity to the exact composition of the sample used to fit the model.

Maximising joint probability superior to maximising average probability. Maximising joint probabilty will help us find the right model. 

Distance to a target. Measuring the *out of sample deviance*

**Information entropy:**
$$
H(p) = -E[log(p_i)] = - \sum_{i=1}^{n} p_ilog(p_i)
$$
Satisfies 3 criteria of measuing uncertainty:

1) Measurement is a continuous measure
2) High number of possible events increases uncertainty
3) Measure of uncertainty should be additive. (e.g. hot or cold and rain or shine - uncertainty over four combinations of these events rain/hot; rain/cold; sun/hot; sun/cold)

```{r}
## R code 6.9
p <- c( 0.3 , 0.7 )
-sum( p*log(p) )
```

**Kullback Leibler divergence (K-L divergence)**

Divergence is the additional uncertainty induced by using probabilities from one distribution to describe another distribution. 

$$
D_{KL}(p, q) = \sum_i p_i(log(p_i) -log(q_i)) = \sum_i p_i log \left(\frac{p_i}{q_i}\right) 
$$

Cross entropy $H(p,q)$ uses probability distribution 1 to predict events of another distribution p. Since divergence is defined as the *additional* entroby induced by using q, divergence is defined as the difference between $H(p)$ (the actual entropy of events) and $H(p,q)$, the additional entropy introduced. 

$$
D_{KL}(p, q) = H(p,q) - H(p) = \sum_i p_i log(q_i) - (- \sum_i log(p_i)) = - \sum_i p_i(log(q_i) - log(p_i))
$$

Multiplying average log probability by -1 makes entropy *H* increase from zero as opposed to decrease from zero.

# Quotes

Stripping away information away about uncertainty leads to over confidence.

Priors information is more subtle, because parameteres don't always have such clear physical meaning.

Posterior probabilities of parameter values describe the relative compatibility of different states of the world with the data, according to the model.

Page 107-108 - Prediction of the mean and uncertainty around mu_hat based on simulating from joint posterior of the linear model (beta_0 + X*beta_1). Actual predictions of y variable depend also on sigma, the stochastic variation from sigma of the likelihood.

"What is the predictive value of a variable, once I already know all of the other predictive variables"

pg 145: Multicollinearity: We may not anticipate a clash between highly correlated predictors. And therefore we may mistakenly read the posterior distribution to say that neither predictor is important.

ph 146: The absolute magnitude of regression slopes is not always meanginful, because the influence on prediction depends upon the product of the parameeter and the data. Compute or plot predictions, unless you standardise the predictors. *Always plot implied predictions*

pg 149 *Check vovariation among parameters, especially when intervals are extremely wide, so you dont' misinterpret what the model is trying to tell you*

pg 152 Pre treatment vaeriables may mask causal inference of treatment *if excluded from the model*, so we should include them in the model.
Post treatment varaiables may mask the treatment itself if *included in the model*, so we should exclude them from the model.

Note the model may give better predictions if we include the post treatment variable, it may fit the sample better. But if we care about causal inference, such models are answering the wrong question. Hence we cannot use information criteria for comparison of model validity in this case. It is measuring the wrong metric.