---
title: "Lecture Notes - Statistical Rethinking"
author: "Benjamin Wee"
output: pdf_document
---

# Lecture 01

Need to understand the internal processes of a model to use them properly. Construction from bottom up.

Your computer is a robot (Golem), it takes in instructions and unfortuantely, it might carry them out. It does exactly what you say, to the letter. Need to understand them to be responsible when building them.

Eventually you will make a statistical model which will misbehave. But it will only misbehave according to your expectations. It is behaving *exactly* according to its design.

Models are blind to the creator's intent, easy to misuse, not even false.

We construct models to do things which are difficult for us. We design them to be good at things we are bad at. But they are fantastically bad at the things people find easy. They are blind to our intentions. 

"Animated by truth"

Frequentist tests are made for experimental designs (or factorial design models?).  

Flasifying null models are insufficient for understanding how the world works.

"Statistical expectation" 

Bayesian inference lays bare all the imperfections of logical inference.

Information critera are a cross validation metric

Anything which looks random to you is because you can't predict the outcome. Ignorance in the face of uncertainty.

Everything in Bayes is pseudo random, there is a deterministic process (probability distributions which produce random numbers). We taught computers to generate pseudo random numbers (we taught them a deterministic way to generate "randomness")

Propositional logic. What is a valid deduction. Bayes extends this to continuous logic.

We want to understand "the golem's belief", not the scientist's.

Bayesian stats -> Laplacian stats. Father of applied stats. Epistemic rather than ontological. Small world, making the best inference we have based on the existing information.

Coins themselves are not random. Our inability to predict their side which makes them random. They are a good randomisation device.

FLipping coins fairly is a chaotic system. So sensitive to initial conditions that they appear unpredictable, and hence randoom.

randomness is a property of bugs in our knowldege.

Based on our assumptions/conjecture, how different ways could we observe the data?

Probability is merely counting the number of ways the data could appear given our conjecture GIVEN all the POSSIBLE paths that could be generated from our conjecture. This is where the multiplication rule of probability comes from. We are multiplying all the possible "paths" of observing the data from our assumptions (an efficient way of counting). 

# Lecture 02

Extension of propositional logic to continuous probability

```{r}
# Counts to probability
ways <- c(3 ,8, 9)
ways/sum(ways)
```

Probability is just a shortcut for counting probabilties. It cannot tell us what is true, nothing can. It's simply, make some assumptions, and see what the logical consequences are. 

Any particular dataset you observe is vanishingly unlikely.

Applied statistics as a type of engineering. Building a bridge across a river of ignorance. 

*Design > Condition > Evaluate*
Story of the small world, executing in the small world, does the model make sense? 

Applied statistics, a form of engineering. Where we will be constructing a bridge to get over a river of ignorance.

Translating data story into probabilty statements:

1) Law of Total Probability: Make sure the probabilities sum to 1
Two to remember to construct things:
2) Sum rule: Things that are alternative: add
3) Product rule: Things that happen together: multiply

The prior and posterior are one in the same. The prior summarises all the past datapoints AND the sample size. May want the raw data IF you want to fit a different model. Other models may find other relevant information.

Posterior is the prior of a different model or distribution and every prior is the posterior of some other inference. This is bayesian updating -- continuous learning bsed on the assumptions built into it

Bayesian learning is optimal in our small world. Counting up the logical ways data could arise. 

Ratio of factorials in binomial distribution gives the number of ways the data can appear. Count the different orders you can get all the permutations.

Bayes: Translate a prior state of information (before data arrives) into posterior state of information (after data arrives)

Prior: We have prior counts of the plausibilities of each conjecture (e.g. philosophy or previous data). 

Prior: assigning information states. Joint prior over data and parameters. 

Prior predictive distribution: Idea of simulated dataset. Terrible predictions before you got data. **This is the way to know what the prior actually means**. *It is the expectation of the dataset before you see the data*

Posterior predictive distribution: hopefully better predictions after data is observed. Good way to test what the golem thinks its learned from the data.

Density: "Rate of change" -- It's continuous. Relative values matters, not absolute.

Overfitting - Need to build in conservatism into golems. Otherwise it will think the sample is "everything"

Prior as a state of information, there's no "true" prior. Important to embody your golem with information it to guard against *inferential risk*. Truth isn't what's relevant here. It's the risk of taking action from the model that matters.

$$
Posterior = \frac{Likelihood \times prior}{Average\space Likelihood}
$$

Where the average likelihood normalises the posterior so that it integrates to 1. The posterior is the *standardised product of the likelihood and proior


# Lecture 03
"Just because your model makes accurate predictions, does not mean it is right." - They're not even wrong. They're descriptions and useful. 

Linear regression is descriptive of how the mean is conditional on variables we know. Does necessarily model the natural process. They are *descriptively accurate* and a good *general method of approximation.*

Gaussian as a model of error.

Gaussian distribution is the *most conservative*. Spreads probabilitly the most *flatly* for a given mean and variance.

Adding fluctuations -> Gaussian, preserves mean and variance. Symmetry arises from the addition process. 

Gaussian is the most conservative asumption if we assume finite variance and most conservative distribution.

_'Learn strategy, not procedure'_

Language for modeling

1) What are the outcomes?
2) How are the outcomes contrained (what is the likelihood)?
3) What are the predictiors, if any
4) How do predictors relate to likelihood
5) What are the priors?

Prior predictive distrbutions. What does the model think before it see the data.

Posterior distribution is the relative count of all the ways the data (we have) can happen conditional on the different combination of parameter values

Margins of distributions -> Looking at the side of the hill.

Statistical models are descriptive and not causal. Don't get these two mixed up.

Sampling is an epistomoligcal device to characterise uncertainty around our estimates. Not a physical assumption. Posterior do not "exist", they're a epistological tool.

# Lecture 04
Characterising uncertainty:

1) Sample from posterior
2) Use samples to generate predictions that 'integrate over uncertainty'

Anything that depends on the posterior distribution is also a distribution

Consider using median when the posterior distribution is skewed.

Reporting results from a model is different from using a model to make a _decision_. 

Uncertainty from posterior (of some parameter, e.g. my) and uncertainty from likelihood gives the distribution of predicted $y$

Uncertainty around the expectation, uncertainty around realised dependent variable.

Interested in the shape of the interval *not* the boundaries.

Use prime intervals. Braek convention of 95\%

Continuous reduction in plausibility the more we extend out.

Linear models are not linear in the prediction space. It's about the additiveness of the model and the mean. 

...descriptively accurate to an arbitrary standard

Best paraobla go fit if you tell the computer to fit a porabola. Can swing wildly in small samples.



# Lecture 05
The causal terror - when variables which are not obviously spurious but really are. Can NOT determine causality alone with data

Adding variables can induce spurious correlations.

People tend to exaggerate the power of statistical methods

Statitsics provides a structured analytical system to clearly communicate what you've done. It does not solve *the* (inference) problem (nothing really does). It solves *communication* problems and *computational* problems.

Causation does not imply correlation. There are complex systems (non linear relationships, delayed feedback mechanisms) which mean causal related variables may exhibit no correlation. Unless you look at it in exactly the right way when revealed by a model how a system works. That is why you need theory. 

Need a model of the system so you know what kind of association to look for.

Zero is a natural inflection point. There can be still an effect even if the interval crosses zero. Do not cave in to conventional superstitions that you should ignore variables because of this and assume that there is no effect 

Residual is not data, residual is a parameter, it's dependent on the posterior distribution.

Goal is not to get a perfect retrodiction of your sample. It's supposed to help you identify areas the model performed badly in so it gives you a way forward + think about what's wrong. 

# Lecture 06
If the model is not exactly retrodicting, it does not mean something is wrong.
The whole point of multilevel model is that they **don't** fit the sample perfectly well. As a consequence they make better predictions out of sample. You expect them to make a pattern of mis retrodiction on the sample which is a feature not a bug.

Could have large effects which are spurious, beware of putting all the variables into your model.

If you do not ask exactly the right question and understand the implications of your question in a precise way, the problem is the model will answer it _exactly. And that answer will burn your kingdom to the ground. So best approximate the answer to the right question which is often vague than an exact answer to the wrong question which can always be made precise.

Can have lots of powerful ML algorithms with lots of variables but it does not help us understand how the underlying system _works_. Not about predictions, it's about understanding and interpretability

"The question you answered isn't exactly the one you really wanted to ask"

If you see variables with very wide porterior intervals, consider plotting them and seeing if there is any mutual information or multicollinearity.

Post treatment bias - Controlling for consequence of treatment statistically knocks out treatment. Don't want to control for consequences of the treatment otherwise we can isolate the treatment effect. The mediator or consequence exists *because* of the treatment effect.
Understanding *how* the treatment worked through mediators requires separate models

If there's something caused by two variables and you control for it, you are looking at a subset -- you are inducing a statistical selection bias on the cases that is informing you about the relationship of the two other variables

Treatments effect a bunch of variables at once, so need to think carefully about what that means.

You always need theories about the systems to solve the issues. Mere associations won't do it.


# Lecture 07
Can't use ochham's razor in most realistic circumstances. Rarely will we compare models that make the _exact_ same predictions. Rather, we have to make a choice between _tradeoffs_. Some model fits the data better but it's much more complicated. We need a way of managing that tradeoff of fit and complexity or fit and prediction.

Overfitting encodes the sample that we have into the model.

Information: Reduction in uncertainty caused by learning an outcome

You understand the winning model better when you _compare_ and _understand_ why the other models performed worse relative to it. Don't discard, use all the information. 

# Lecture 08
A lot of real life inference/prediction problems or learnings don't allow us to train and then test out of sample. Some decision problems require gathering data as you learn. 

Prequential analysis - Data collection strategy is mixed in with the estimation strategy. Data collection is guided by what we just learned. 

'Prequential experimental design'

Actual parameters vs effective number of parameters.
Parameters all have different relationships to one another
 
Not all parameters have direct relationships to prediction — they have to pass through other parameters
 
Measuring the quantitative distances. Not about 'which model is the best'. We learn more from _comparing_ models and inspecting their differences by their relative distances.
 
 Never average parameters. Only average and ensemble predictions. Information criteria are measured on the prediction scale.
 
 You understand the winning model _better_ when you understand the other models relative to it
 
 dSE -> Standard error of the DIFFERENCE in the distributions of WAICs.
 
 WAICs have SEs and therefore an expected sampling distribution of WAIC implied by the model. 
 
 Comparing two different models: there are two distributions for WAIC and those distributions are _correlated_
 
 Since they are correlated, the distribution of their difference is NOT the same as the difference in their distributions
 
 Don't look at the overlap of SE bars and infer whether two models are different from one another. In any statistical model comparison, you do NOT just look at the overlap in error bars. This is irrelevant to the question or whether the estimates are different from one another.
 
 If dSE crosses zero, might suggest distributions in WAIC overlap
 
 Visualise parameter distributions across models to identify masking effects
 
 Irreconcilable differences in the structure of their models -> average their predictions together.
 
 NEVER average parameter estimates
 
 WAIC is measured on prediction space so its okay to average predictions. Predictions is what the information criteria is judging on.
 
 Only looking at top model under calibrates the risk. Ensemble model does not sweep anything under the rug.
 

# Lecture 09
We're mislead because our sample is _conditional_ on something, it's a subset or subsample of the population

Splitting data is a bad idea -- does not pool information. Model does not learn from the whole sample.

Dummy variables _only_ shifts the intercepts and gives the same slope
 
Don't interpret coefficients. Interactions now rely on more than one parameters. Need to compute the total slope.
 
Look at the distribution of their difference.

Always include all the dummy variables, otherwise you understate uncertainty (i.e get rid of the alpha intercept term)

# Quotes
_'Learn strategy, not procedure'_ L03 19mins

We laugh because the alternative is to cry L04 47 mins

Causation does not imply correlation. There are complex systems (non linear relationships, delayed feedback mechanisms) which mean causal related variables may exhibit no correlation. Unless you look at it in exactly the right way when revealed by a model how a system works. That is why we need theory. L05 10mins

Zero is a natural inflection point. There can be still an effect even if the interval crosses zero. Do not cave in to conventional superstitions that you should ignore variables because of this and assume that there is no effect 
 L05 25 mins
 
If you do not ask exactly the right question and understand the implications of your question in a precise way, the problem is the model will answer it _exactly. And that answer will burn your kingdom to the ground.  L06 28 mins
> So best approximate the answer to the right question which is often vague than an exact answer to the wrong question which can always be made precise.
 
Information entropy: Two words that we don't understand and we put them together L0724m. BW: Ensembling

# Ideas
Causal inference: Use Information Criteria
Prediction: Test out of sample

R^2 not useful in either circumstance. Can have good R^2 but misspecified model. Can have bad R^2 but the _correct_ model. 
In prediction, fit to the sample is _supposed_ to not be good, it's out of sample that matters.