---
title: "Lecture Notes - Statistical Rethinking"
author: "Benjamin Wee"
output: pdf_document
---

# Lecture 01

Need to understand the internal processes of a model to use them properly. Construction from bottom up.

Your computer is a robot (Golem), it takes in instructions and unfortuantely, it might carry them out. It does exactly what you say, to the letter. Need to understand them to be responsible when building them.

Eventually you will make a statistical model which will misbehave. But it will only misbehave according to your expectations. It is behaving *exactly* according to its design.

Models are blind to the creator's intent, easy to misuse, not even false.

We construct models to do things which are difficult for us. We design them to be good at things we are bad at. But they are fantastically bad at the things people find easy. They are blind to our intentions. 

"Animated by truth"

Frequentist tests are made for experimental designs (or factorial design models?).  

Flasifying null models are insufficient for understanding how the world works.

"Statistical expectation" 

Bayesian inference lays bare all the imperfections of logical inference.

Information critera are a cross validation metric

Anything which looks random to you is because you can't predict the outcome. Ignorance in the face of uncertainty.

Everything in Bayes is pseudo random, there is a deterministic process (probability distributions which produce random numbers). We taught computers to generate pseudo random numbers (we taught them a deterministic way to generate "randomness")

Propositional logic. What is a valid deduction. Bayes extends this to continuous logic.

We want to understand "the golem's belief", not the scientist's.

Bayesian stats -> Laplacian stats. Father of applied stats. Epistemic rather than ontological. Small world, making the best inference we have based on the existing information.

Coins themselves are not random. Our inability to predict their side which makes them random. They are a good randomisation device.

FLipping coins fairly is a chaotic system. So sensitive to initial conditions that they appear unpredictable, and hence randoom.

randomness is a property of bugs in our knowldege.

Based on our assumptions/conjecture, how different ways could we observe the data?

Probability is merely counting the number of ways the data could appear given our conjecture GIVEN all the POSSIBLE paths that could be generated from our conjecture. This is where the multiplication rule of probability comes from. We are multiplying all the possible "paths" of observing the data from our assumptions (an efficient way of counting). 

# Lecture 02

Extension of propositional logic to continuous probability

```{r}
# Counts to probability
ways <- c(3 ,8, 9)
ways/sum(ways)
```

Probability is just a shortcut for counting probabilties. It cannot tell us what is true, nothing can. It's simply, make some assumptions, and see what the logical consequences are. 

Any particular dataset you observe is vanishingly unlikely.

Applied statistics as a type of engineering. Building a bridge across a river of ignorance. 

*Design > Condition > Evaluate*
Story of the small world, executing in the small world, does the model make sense? 

Applied statistics, a form of engineering. Where we will be constructing a bridge to get over a river of ignorance.

Translating data story into probabilty statements:

1) Law of Total Probability: Make sure the probabilities sum to 1
Two to remember to construct things:
2) Sum rule: Things that are alternative: add
3) Product rule: Things that happen together: multiply

The prior and posterior are one in the same. The prior summarises all the past datapoints AND the sample size. May want the raw data IF you want to fit a different model. Other models may find other relevant information.

Posterior is the prior of a different model or distribution and every prior is the posterior of some other inference. This is bayesian updating -- continuous learning bsed on the assumptions built into it

Bayesian learning is optimal in our small world. Counting up the logical ways data could arise. 

Ratio of factorials in binomial distribution gives the number of ways the data can appear. Count the different orders you can get all the permutations.

Bayes: Translate a prior state of information (before data arrives) into posterior state of information (after data arrives)

Prior: We have prior counts of the plausibilities of each conjecture (e.g. philosophy or previous data). 

Prior: assigning information states. Joint prior over data and parameters. 

Prior predictive distribution: Idea of simulated dataset. Terrible predictions before you got data. **This is the way to know what the prior actually means**. *It is the expectation of the dataset before you see the data*

Posterior predictive distribution: hopefully better predictions after data is observed. Good way to test what the golem thinks its learned from the data.

Density: "Rate of change" -- It's continuous. Relative values matters, not absolute.

Overfitting - Need to build in conservatism into golems. Otherwise it will think the sample is "everything"

Prior as a state of information, there's no "true" prior. Important to embody your golem with information it to guard against *inferential risk*. Truth isn't what's relevant here. It's the risk of taking action from the model that matters.

$$
Posterior = \frac{Likelihood \times prior}{Average\space Likelihood}
$$

Where the average likelihood normalises the posterior so that it integrates to 1. The posterior is the *standardised product of the likelihood and proior


# Lecture 03
"Just because your model makes accurate predictions, does not mean it is right." - They're not even wrong. They're descriptions and useful. 

Linear regression is descriptive of how the mean is conditional on variables we know. Does necessarily model the natural process. They are *descriptively accurate* and a good *general method of approximation.*

Gaussian as a model of error.

Gaussian distribution is the *most conservative*. Spreads probabilitly the most *flatly* for a given mean and variance.

Adding fluctuations -> Gaussian, preserves mean and variance. Symmetry arises from the addition process. 

Gaussian is the most conservative asumption if we assume finite variance and most conservative distribution.

_'Learn strategy, not procedure'_

Language for modeling

1) What are the outcomes?
2) How are the outcomes contrained (what is the likelihood)?
3) What are the predictiors, if any
4) How do predictors relate to likelihood
5) What are the priors?

Prior predictive distrbutions. What does the model think before it see the data.

Posterior distribution is the relative count of all the ways the data (we have) can happen conditional on the different combination of parameter values

Margins of distributions -> Looking at the side of the hill.

Statistical models are descriptive and not causal. Don't get these two mixed up.

Sampling is an epistomoligcal device to characterise uncertainty around our estimates. Not a physical assumption. Posterior do not "exist", they're a epistological tool.

# Lecture 04
Characterising uncertainty:

1) Sample from posterior
2) Use samples to generate predictions that 'integrate over uncertainty'

Anything that depends on the posterior distribution is also a distribution

Consider using median when the posterior distribution is skewed.

Reporting results from a model is different from using a model to make a _decision_. 

Uncertainty from posterior (of some parameter, e.g. my) and uncertainty from likelihood gives the distribution of predicted $y$

Uncertainty around the expectation, uncertainty around realised dependent variable.

Interested in the shape of the interval *not* the boundaries.

Use prime intervals. Braek convention of 95\%

Continuous reduction in plausibility the more we extend out.

Linear models are not linear in the prediction space. It's about the additiveness of the model and the mean. 

...descriptively accurate to an arbitrary standard

Best paraobla go fit if you tell the computer to fit a porabola. Can swing wildly in small samples.



# Lecture 05
The causal terror - when variables which are not obviously spurious but really are. Can NOT determine causality alone with data

Adding variables can induce spurious correlations.

People tend to exaggerate the power of statistical methods

Statitsics provides a structured analytical system to clearly communicate what you've done. It does not solve *the* (inference) problem (nothing really does). It solves *communication* problems and *computational* problems.

Causation does not imply correlation. There are complex systems (non linear relationships, delayed feedback mechanisms) which mean causal related variables may exhibit no correlation. Unless you look at it in exactly the right way when revealed by a model how a system works. That is why you need theory. 

Need a model of the system so you know what kind of association to look for.

Zero is a natural inflection point. There can be still an effect even if the interval crosses zero. Do not cave in to conventional superstitions that you should ignore variables because of this and assume that there is no effect 

Residual is not data, residual is a parameter, it's dependent on the posterior distribution.

Goal is not to get a perfect retrodiction of your sample. It's supposed to help you identify areas the model performed badly in so it gives you a way forward + think about what's wrong. 

# Lecture 06
If the model is not exactly retrodicting, it does not mean something is wrong.
The whole point of multilevel model is that they **don't** fit the sample perfectly well. As a consequence they make better predictions out of sample. You expect them to make a pattern of mis retrodiction on the sample which is a feature not a bug.

Could have large effects which are spurious, beware of putting all the variables into your model.

If you do not ask exactly the right question and understand the implications of your question in a precise way, the problem is the model will answer it _exactly. And that answer will burn your kingdom to the ground. So best approximate the answer to the right question which is often vague than an exact answer to the wrong question which can always be made precise.

Can have lots of powerful ML algorithms with lots of variables but it does not help us understand how the underlying system _works_. Not about predictions, it's about understanding and interpretability

"The question you answered isn't exactly the one you really wanted to ask"

If you see variables with very wide porterior intervals, consider plotting them and seeing if there is any mutual information or multicollinearity.

Post treatment bias - Controlling for consequence of treatment statistically knocks out treatment. Don't want to control for consequences of the treatment otherwise we can isolate the treatment effect. The mediator or consequence exists *because* of the treatment effect.
Understanding *how* the treatment worked through mediators requires separate models

If there's something caused by two variables and you control for it, you are looking at a subset -- you are inducing a statistical selection bias on the cases that is informing you about the relationship of the two other variables

Treatments effect a bunch of variables at once, so need to think carefully about what that means.

You always need theories about the systems to solve the issues. Mere associations won't do it.

# Quotes
_'Learn strategy, not procedure'_ L03 19mins

We laugh because the alternative is to cry L04 47 mins

Causation does not imply correlation. There are complex systems (non linear relationships, delayed feedback mechanisms) which mean causal related variables may exhibit no correlation. Unless you look at it in exactly the right way when revealed by a model how a system works. That is why we need theory. L05 10mins

Zero is a natural inflection point. There can be still an effect even if the interval crosses zero. Do not cave in to conventional superstitions that you should ignore variables because of this and assume that there is no effect 
 L05 25 mins
 
If you do not ask exactly the right question and understand the implications of your question in a precise way, the problem is the model will answer it _exactly. And that answer will burn your kingdom to the ground.  L06 28 mins
> So best approximate the answer to the right question which is often vague than an exact answer to the wrong question which can always be made precise.
